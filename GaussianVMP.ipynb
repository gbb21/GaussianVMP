{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Demo for Variational Message Passing\n",
    "====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Bayesian inference is one of the widely established foundation for machine learning. While exact inference mostly intractble, approximation appraoches like *Monte Carlo method*, *expectation propagation (EP)* and *variational message passing (VMP)* performs iterative local inferences on decomposed graph.\n",
    "\n",
    "*Monte Carlo* method suffers from intensive computation costs, which is usually intractible in practical settings. *EP* and *VMP* are very similar in the way that both of them attempt to optimize a proposed distribution $Q(X)$ to match the posterior distribution $P(X|D)$. The differences between them are following:\n",
    "\n",
    "* *expecation propagation* optimize for the \"inclusive\" divergence $ \\mathbf{KL}(P(X|D)\\ ||\\ Q(X)) $\n",
    "    1. It uses approximation to summarize the entire posterior and ignores the modes. If the posterior is complex, with multiple conflicting solutions, this can lead to very broad approximations or non-convergence;\n",
    "    2. Optimization may assign probability to all plausible outcomes, even when they only have zero probability (zero avoiding).\n",
    "\n",
    "* *variational message passing* optimize for the \"exclusive\" divergence $ \\mathbf{KL}(Q(X)\\ ||\\ P(X|D) $\n",
    "    1. It is similar to EM and also garuntee to converge to some solution, as it optimizing for the evidence lower bound;\n",
    "    2. Optimization may overly confidently fit into one mode of the distribution, but less likely to assign probability to zero probability regions (zero forcing)\n",
    "\n",
    "![Zero forcing and zero avoiding](zeroforcing.png)\n",
    "\n",
    "There are also some approches like *Power EP* that flexible enough to cover both sides, and you may find more infomation in the reference materials at the end of this notebook.\n",
    "\n",
    "We already have mature inference libraries for methods above, like [Infer.NET](https://dotnet.github.io/infer/default.html), or [Bayspy](https://pypi.org/project/bayespy/) in Python. But they have multiple complicated abstraction layers that hides the core inference algorithm behind, which is not easy to read and to undestand.\n",
    "\n",
    "This notebook forces on using miniature examples to demostrate basic *VMP* algorithm over gaussian distributions. It can chain multiple gaussian or gamma random varaibles to form a basic bayesian graph, and solve the posterial based on given observations and prior distribution. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tc\n",
    "import scipy as sp\n",
    "import scipy.special as sps\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Variable* and *Distribution* lays the fundation of the miniature *VMP* framework. *Variable* are the nodes on the Bayesian graph and it can either be a constant value (used as hyperparameter in the prior distribution) or a random variable over a given distribution. *Distribution* is the base class for different distribution types. The supported sub-classes are in exponential family like Gaussian distribution, Gamma distribution.\n",
    "\n",
    "*VMP* requires its distributions in the graph follows the conjugate rules, which is the prior distribution shall have the same function form as its posterior distribution.\n",
    "\n",
    "The message passing inferences has *forward* and *backward* 2 stages, they will do inference along the DAG direction or passing messages backwards respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable includes hyper parmeters (constant) or parameters (based on distributions)\n",
    "class Variable:\n",
    "    def __init__(self, _name:str, _dist):\n",
    "        self.name = _name\n",
    "        # self.expectedValue is either const value (hyper-param) or a distribution's nature stats vector\n",
    "        self.expectedValue = None\n",
    "        self.dist = None\n",
    "        self.refDists = list()\n",
    "        self.messages = list()\n",
    "        self.observation = None\n",
    "        self.version = 0\n",
    "        if isinstance(_dist, Distribution):\n",
    "            _dist.registerOutputVariable(self)\n",
    "        elif isinstance(_dist, (int, float)):\n",
    "            self.expectedValue = _dist\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Variable Name: {0}, Distribution: {1}, Expectation: {2}\".format(self.name, (type(self.dist).__name__ if self.dist else \"point-mass\"), self.getExpValue())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<{0}:{1}={2}>\".format(self.name, (type(self.dist).__name__ if self.dist else \"point-mass\"), self.getExpValue())\n",
    "    \n",
    "    # is the sink of the inference DAG\n",
    "    def isSink(self):\n",
    "        return self.observation is not None and not self.refDists\n",
    "    \n",
    "    # is the source of the inference DAG\n",
    "    def isSource(self):\n",
    "        return self.expectedValue is not None and self.dist is None\n",
    "    \n",
    "    # reset the inference state\n",
    "    def resetState(self):\n",
    "        self.version = 0\n",
    "        self.messages.clear()\n",
    "        if self.dist:\n",
    "            self.expectedValue = None\n",
    "    \n",
    "    # provide observed value of the variable\n",
    "    def observe(self, _evidence):\n",
    "        if not self.dist:\n",
    "            raise ValueError(\"You cannot observe constants\")\n",
    "        self.observation = _evidence\n",
    "    \n",
    "    # being depended on as input parameter for a distribution\n",
    "    def reference(self, _dist):\n",
    "        self.refDists.append(_dist)\n",
    "    \n",
    "    # get expectation of the variable\n",
    "    # only can be invoked after inference\n",
    "    def getExpValue(self):\n",
    "        return self.expectedValue[0] if isinstance(self.expectedValue, list) else self.expectedValue\n",
    "    \n",
    "    # update the variable from its underneath distribution\n",
    "    def _pullForward(self, _version):\n",
    "        if self.dist and self.version < _version:\n",
    "            self.dist._forward(_version)\n",
    "            self.version = _version\n",
    "        self.messages.clear()\n",
    "            \n",
    "        return self.version\n",
    "    \n",
    "    # update the variable from distributions referenced it\n",
    "    def _pullBackward(self, _version):\n",
    "        if self.version < _version:\n",
    "            for ref in self.refDists:\n",
    "                ref._backward(_version)            \n",
    "            if self.dist:\n",
    "                self.version = _version\n",
    "\n",
    "        return self.version\n",
    "\n",
    "# Distribution base class for gaussian message passing inference\n",
    "class Distribution:\n",
    "    def __init__(self, _inputNames, _outputName):\n",
    "        self.params = dict()\n",
    "        self.inputs = _inputNames\n",
    "        self.output = _outputName\n",
    "    \n",
    "    # sub-class should provide nature stats vector calculations based on 3 cases\n",
    "    # 1. variable was observed\n",
    "    # 2. variable was not observed, without backward updating messages\n",
    "    # 3. variable was not observed, with backward updating messages\n",
    "    def calcNatureStats(self, _messages):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "    \n",
    "    # create backward updating messages for its parameters: _target\n",
    "    # sub-class should provide corresponding messages based on different types of its parameters\n",
    "    def calcBackwardMessage(self, _target):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "    \n",
    "    # assign the output parameter to the distribution\n",
    "    def registerOutputVariable(self, _var):\n",
    "        self.params[self.output] = _var\n",
    "        _var.dist = self\n",
    "    \n",
    "    # assign the input parameter to the distribution\n",
    "    def registerInputVariable(self, _name, _var):\n",
    "        if _name not in self.inputs:\n",
    "            raise ValueError(\"parameter name {0} not found\".format(_name))\n",
    "        self.params[_name] = _var\n",
    "        _var.reference(self)\n",
    "    \n",
    "    # forward passing messages to its output variable\n",
    "    def _forward(self, _version):\n",
    "        if self.params[self.output].version < _version:\n",
    "            vers_ = 0\n",
    "            for p in self.inputs:\n",
    "                vers_ = max(vers_, self.params[p]._pullForward(_version))\n",
    "            \n",
    "            if self.params[self.output].version <= vers_:\n",
    "                self.params[self.output].expectedValue = self.calcNatureStats(None)\n",
    "        \n",
    "            self.params[self.output].version = _version\n",
    "\n",
    "    # backward passing messages to its parameter variables\n",
    "    def _backward(self, _version):\n",
    "        if self.params[self.output].version < _version:\n",
    "            self.params[self.output]._pullBackward(_version)\n",
    "            self.params[self.output].expectedValue = self.calcNatureStats(self.params[self.output].messages)\n",
    "        \n",
    "        for t in self.inputs:\n",
    "            m = self.calcBackwardMessage(t)\n",
    "            if m is not None:\n",
    "                self.params[t].messages.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we implemented 2 distributions *Gaussian* and *Gamma*. Their conditional probability given parents (parameters) all can be easilly written into a *multi-linear* form in respect to the random variable and their parameters, as instructioned in the paper [1], which greatly simplified the steps to compute varitional messages. \n",
    "\n",
    "In particular, supported distribution in the graph can be written into the form of\n",
    "$$ \\mathbf{ln} P(Y|pa_y) = \\phi_y (pa_y) \\mathbf{u}_y(Y) + f_y(Y) + g_y(pa_y) $$\n",
    "where $Y$ is the random variable, $pa_y$ are its parient nodes. We have $\\phi_y(pa_y)$ as the nature parameter of the distribution and $\\mathbf{u}_y(Y)$ as the nature stats of the distribution.\n",
    "\n",
    "The distributions bellow shall provide 2 values:\n",
    "1. The expectation of the random variable - which can be given by $\\frac{\\mathbf{d}\\hat{g}(\\phi)}{\\mathbf{d}\\phi}$, which $\\mathbf{d}\\phi$ is the $\\phi_y$ re-parametered function.\n",
    "2. The backward messages to their parameters - which requires re-write their *multi-linear* density function into the forms of the density functions of their parent nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gamma distribution without bayesian input parameters\n",
    "class GammaDistribution(Distribution):    \n",
    "    _ParamAlpha = \"alpha\"\n",
    "    _ParamBeta = \"beta\"\n",
    "    _ParamGamma = \"gamma\"\n",
    "    \n",
    "    def __init__(self, _alpha, _beta):\n",
    "        super().__init__([GammaDistribution._ParamAlpha, GammaDistribution._ParamBeta], GammaDistribution._ParamGamma)\n",
    "        if _alpha.dist:\n",
    "            raise ValueError(\"parameter alpha must be point-mass distribution\")\n",
    "        self.registerInputVariable(GammaDistribution._ParamAlpha, _alpha)\n",
    "        if _beta.dist:\n",
    "            raise ValueError(\"parameter beta must be point-mass distribution\")\n",
    "        self.registerInputVariable(GammaDistribution._ParamBeta, _beta)\n",
    "        \n",
    "    def calcNatureStats(self, _messages):\n",
    "        if self.params[self.output].observation is not None:\n",
    "            val_ = self.params[self.output].observation            \n",
    "            return [val_, math.log(val_)]\n",
    "        elif _messages:\n",
    "            a = -self.params[GammaDistribution._ParamBeta].expectedValue + sum(map(lambda x: x[0], _messages))\n",
    "            b = self.params[GammaDistribution._ParamAlpha].expectedValue - 1 + sum(map(lambda x: x[1], _messages))\n",
    "            a, b = b + 1, -a\n",
    "            # E(ln(x)) = digamma(a) - ln(b))\n",
    "            return [a / b, sps.digamma(a) - math.log(b)]\n",
    "        else:\n",
    "            a = self.params[GammaDistribution._ParamAlpha].expectedValue\n",
    "            b = self.params[GammaDistribution._ParamBeta].expectedValue            \n",
    "            # E(ln(x)) = digamma(a) - ln(b))\n",
    "            return [a / b, sps.digamma(a) - math.log(b)]\n",
    "    \n",
    "    # No backward message are all non-bayesian variables\n",
    "    def calcBackwardMessage(self, _target):\n",
    "        pass\n",
    "\n",
    "# the normal distribution (with bayesian input parameters)\n",
    "class NormalDistribution(Distribution):\n",
    "    _ParamMu = \"mu\"\n",
    "    _ParamGamma = \"gm\"\n",
    "    _ParamX = \"x\"\n",
    "    \n",
    "    def __init__(self, _mu = None, _gamma = None):\n",
    "        super().__init__([NormalDistribution._ParamMu, NormalDistribution._ParamGamma], NormalDistribution._ParamX)\n",
    "        if _mu.dist and not isinstance(_mu.dist, NormalDistribution):\n",
    "            raise ValueError(\"parameter mu must be {0} or point-mass distribution\".format(NormalDistribution.__name__))\n",
    "        self.registerInputVariable(NormalDistribution._ParamMu, _mu)\n",
    "        if _gamma.dist and not isinstance(_gamma.dist, GammaDistribution):\n",
    "            raise ValueError(\"parameter Garmma must be {0} or point-mass distribution\".format(GammaPriorDistribution.__name__))\n",
    "        self.registerInputVariable(NormalDistribution._ParamGamma, _gamma)\n",
    "    \n",
    "    def calcNatureStats(self, _messages):\n",
    "        if self.params[self.output].observation is not None:\n",
    "            val_ = self.params[self.output].observation\n",
    "            return [val_, val_ * val_]        \n",
    "        elif not _messages:\n",
    "            mu_ = self.params[NormalDistribution._ParamMu].expectedValue\n",
    "            if isinstance(mu_, list):\n",
    "                return mu_\n",
    "            else:\n",
    "                # E(x^2) = E(x)^2 + Phi(x)^2\n",
    "                phi2_ = 1 / self.params[NormalDistribution._ParamGamma].getExpValue()\n",
    "                return [mu_, mu_ * mu_ + phi2_]\n",
    "        else:\n",
    "            # E(x) = mu = -a/(2b)\n",
    "            # mu^2 = a^2/(4b^2)\n",
    "            # gamma = -2b\n",
    "            # E(x^2) = mu^2 + 1/gamma = (a^2-2b)/(4b^2)\n",
    "            a = sum(map(lambda x: x[0], _messages)) + self.params[NormalDistribution._ParamMu].getExpValue() * self.params[NormalDistribution._ParamGamma].getExpValue()\n",
    "            b = sum(map(lambda x: x[1], _messages)) - self.params[NormalDistribution._ParamGamma].getExpValue() / 2\n",
    "            return [-a / (2 * b), (a * a - 2 * b) / (4 * b * b)]\n",
    "    \n",
    "    def calcBackwardMessage(self, _target):\n",
    "        if _target == NormalDistribution._ParamMu:            \n",
    "            # case of non-bayesian parameter\n",
    "            if not self.params[NormalDistribution._ParamMu].dist:\n",
    "                return None\n",
    "            # parameter is normal distribution\n",
    "            return [self.params[NormalDistribution._ParamGamma].getExpValue() * self.params[NormalDistribution._ParamX].expectedValue[0], -self.params[NormalDistribution._ParamGamma].getExpValue() / 2]\n",
    "        \n",
    "        elif _target == NormalDistribution._ParamGamma:\n",
    "            # case of non-bayesian parameter            \n",
    "            if not self.params[NormalDistribution._ParamGamma].dist:\n",
    "                return None\n",
    "            # parameter is gamma distribution\n",
    "            mu2Exp_ = self.params[NormalDistribution._ParamMu].expectedValue[1] if isinstance(self.params[NormalDistribution._ParamMu].expectedValue, list) else self.params[NormalDistribution._ParamMu].expectedValue * self.params[NormalDistribution._ParamMu].expectedValue\n",
    "            return [-(self.params[NormalDistribution._ParamX].expectedValue[1] + mu2Exp_) / 2 + self.params[NormalDistribution._ParamX].expectedValue[0] * self.params[NormalDistribution._ParamMu].getExpValue(), 0.5]\n",
    "        else:\n",
    "            raise ValueError(\"parameter {0} not found\".format(_target))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the simple inference function to solve the hidden variable's posterior expectations, given the graph structure and observations.\n",
    "\n",
    "It takes multiple iterations of *forward* and *backward* message passing steps, until the target variable converges under certain error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference action for a given graph (list of chained variable),\n",
    "# and query (the variable targeted for posterial)\n",
    "# the return value is the expectation of the target variable\n",
    "def inference(_graph, _query, _eps = 1E-5):\n",
    "    if _eps <= 0:\n",
    "        raise ValueError(\"parameter eps must be greater than 0\")\n",
    "    sinks_ = list(filter(Variable.isSink, _graph))\n",
    "    sources_ = list(filter(Variable.isSource, _graph))\n",
    "    for p in _graph:\n",
    "        p.resetState()\n",
    "    delta_ = _eps + 1\n",
    "    i = 0\n",
    "    lastVal_ = None\n",
    "    while delta_ > _eps:\n",
    "        i += 1\n",
    "        for s in sinks_:\n",
    "            s._pullForward(i)\n",
    "        i += 1\n",
    "        for s in sources_:\n",
    "            s._pullBackward(i)\n",
    "        if lastVal_ is not None:\n",
    "            delta_ = abs(_query.getExpValue() - lastVal_)\n",
    "        lastVal_ = _query.getExpValue()\n",
    "        print(\"iter {0}: exp = {1}\".format(i // 2, lastVal_))\n",
    "    return lastVal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code builds a simple demo chain graph. $y$ is a hidden gaussian variable with its *precision* parameter in gamma distribution. It also derives gaussian varible $x$ as its *mean* parameter. The $x$ variable has 2 observations. The inference solution is for $E(Y|X = x_1, x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = Variable(\"Alpha\", 10)\n",
    "b = Variable(\"Beta\", 1)\n",
    "g1 = Variable(\"GammaY\", GammaDistribution(a, b))\n",
    "g2 = Variable(\"GammaX\", 5.0)\n",
    "m = Variable(\"Mu\", -10)\n",
    "y = Variable(\"Y\", NormalDistribution(m, g1))\n",
    "x1 = Variable(\"X1\", NormalDistribution(y, g2))\n",
    "x2 = Variable(\"X2\", NormalDistribution(y, g2))\n",
    "x1.observe(1)\n",
    "x2.observe(5)\n",
    "graph_ = [a, b, g1, g2, m, y, x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: exp = -3.5\n",
      "iter 2: exp = 2.4116379310344827\n",
      "iter 3: exp = 2.8274818617172435\n",
      "iter 4: exp = 2.8382154765027456\n",
      "iter 5: exp = 2.8384792504300926\n",
      "iter 6: exp = 2.8384857245040926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.8384857245040926"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(graph_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] [Bishop, C. M., & Winn, J. (2006). Variational message passing. Journal of Machine Learning Research, 6(1), 661](https://www.microsoft.com/en-us/research/publication/variational-message-passing/)\n",
    "\n",
    "[2] [Minka, T. P. (2005). Divergence measures and message passing. Microsoft Research Technical Report, (MSR-TR-2005-173)](https://www.seas.harvard.edu/courses/cs281/papers/minka-divergence.pdf)\n",
    "\n",
    "[3] [Working with different inference algorithms\n",
    "](https://dotnet.github.io/infer/userguide/Working%20with%20different%20inference%20algorithms.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
